{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Factorizable Library generation\n",
        "\n",
        "This notebook serves as a demo for training models for use in generating Stochastically Annealed Product Space (SAPS) libraries as presented by _Dai & Saksena et. al. 2022_. Here, an example of generating a library to optimize ranibizumab affinity selection data is presented. "
      ],
      "metadata": {
        "id": "NlQS2Mo64pBo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5NEtmgSpYUT"
      },
      "source": [
        "## Brief background on Google Colab\n",
        "\n",
        "Google Colab is a free cloud-based service that combines the interactive execution environment of Jupyter Notebooks with cloud compute. A good introduction to Google Colab can be found [here](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
        "\n",
        "Google Colab notebooks can be used with GPU acceleration. If you haven't already, make a copy of the notebook to your Drive by going to File->Save A Copy In Drive so that you can edit the notebook. To enable GPU acceleration, go to Runtime->Change runtime type and select GPU under Hardware accelerator. In the top right corner, there should also be a green tick indicating you are connected to a hosted runtime. If not, click to reconnect. More information is available [here](https://colab.research.google.com/notebooks/gpu.ipynb)\n",
        "\n",
        "Then, if correctly set up, `nvidia-smi` should return information about the available GPU. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VCStwuo2G5I",
        "outputId": "e5fac96e-f3b3-4353-b592-0eded2585bd2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  1 07:06:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sajivsaksena/BME350.git\n",
        "%cd "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbBdhrGV2Hz8",
        "outputId": "6deea711-0a53-42fb-8a2a-237454f2dd44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FactorizableLibrary'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 412 (delta 25), reused 87 (delta 25), pack-reused 324\u001b[K\n",
            "Receiving objects: 100% (412/412), 292.15 MiB | 38.12 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "Checking out files: 100% (317/317), done.\n",
            "/content/FactorizableLibrary/library_generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import time\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pprint"
      ],
      "metadata": {
        "id": "VvL_jL4z28DA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.set_device(0)"
      ],
      "metadata": {
        "id": "zTPxVnfA65ML"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition for library scoring\n",
        "- See TrainModel.ipynb for additional details"
      ],
      "metadata": {
        "id": "3m-u8ons8iP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, dilation):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
        "        self.conv1_bn = nn.BatchNorm1d(channels)\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
        "        self.conv2_bn = nn.BatchNorm1d(channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = F.relu( self.conv1_bn( self.conv1(x) ) )\n",
        "        return self.conv2_bn( self.conv2(x1) ) + x\n",
        "\n",
        "class EmbedderNet(nn.Module):\n",
        "    def __init__(self, length, channels, outchannels):\n",
        "        super(EmbedderNet, self).__init__()\n",
        "        self.length = length\n",
        "        \n",
        "        self.pool = nn.MaxPool1d(2, 2, ceil_mode = True)\n",
        "        self.conv = nn.Conv1d(40, channels, 1, 1, 0)\n",
        "        self.conv_bn = nn.BatchNorm1d(channels)\n",
        "        \n",
        "        self.block1 = ResBlock(channels, 1)\n",
        "        self.block2 = ResBlock(channels, 1)\n",
        "        self.block3 = ResBlock(channels, 1)\n",
        "        self.block4 = ResBlock(channels, 1)\n",
        "        self.block5 = ResBlock(channels, 1)\n",
        "        \n",
        "        self.embed_1 = nn.Linear( int( channels*math.ceil(math.ceil(self.length/2)/2) ) , 128)\n",
        "        self.embed_2 = nn.Linear(128, outchannels)\n",
        "\n",
        "        # deprecated positional encoding\n",
        "        position = ( (torch.tensor([i for i in range(self.length)]).view(-1,1)\n",
        "                      - torch.tensor([i for i in range(self.length)]).view(1,-1)) )\n",
        "        stack = []\n",
        "        for i in range(-self.length+1, self.length):\n",
        "            stack.append((position == i).float())\n",
        "        positioncode = torch.stack(stack).view(1,(2*self.length-1),self.length,self.length)\n",
        "        \n",
        "        self.register_buffer(\"position\", positioncode)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "        x = F.relu( self.conv_bn( self.conv(x) ) )\n",
        "        \n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.pool( self.block4(x) )\n",
        "        x = self.pool( self.block5(x) ).view(batchsize,-1)\n",
        "        return self.embed_2( F.relu( self.embed_1(x) ) )\n",
        "    \n",
        "class Predictor_Joint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Predictor_Joint, self).__init__()\n",
        "        self.embedSeed = EmbedderNet(10, 64, 16)\n",
        "       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        seed = x\n",
        "  \n",
        "        return torch.sum(self.embedSeed(seed), dim = 1)"
      ],
      "metadata": {
        "id": "gxYD9Cv-3CcV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetEnsemble(nn.Module):\n",
        "    def __init__(self, lst):\n",
        "        super(NetEnsemble, self).__init__()\n",
        "        self.nets = nn.ModuleList(lst)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return torch.stack([net(x) for net in self.nets]).permute((1,0,2))\n",
        "\n",
        "class Combine(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Combine, self).__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        return torch.einsum(\"nij,nij->n\",x1,x2)\n",
        "        \n",
        "    def getPrefix(self, x1, x2):\n",
        "        return torch.einsum(\"nij,ij->n\",x1,x2)\n",
        "    \n",
        "    def getSuffix(self, x1, x2):\n",
        "        return torch.einsum(\"ij,nij->n\",x1,x2)"
      ],
      "metadata": {
        "id": "nt0ql9xj3IpL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass in a list of models to get an ensemble\n",
        "def loadModels(fns):\n",
        "    seedMaps = []\n",
        "\n",
        "    for fn in fns:\n",
        "        net = Predictor_Joint()\n",
        "        net.load_state_dict(torch.load( fn, map_location=\"cuda:0\"))\n",
        "        seedMaps.append(net.embedSeed)\n",
        "    seedEnsemble = NetEnsemble(seedMaps)\n",
        "    combine = Combine()\n",
        "    seedEnsemble.eval().cuda()\n",
        "    combine.eval()\n",
        "    return seedEnsemble, combine\n",
        "\n",
        "def calibrate(model):\n",
        "    with open(\"../model_training/encoding.pkl\", 'rb') as fin:\n",
        "        encoding = pickle.load(fin)\n",
        "    seed = []\n",
        "    with open(\"calibration.txt\", 'rt') as fin:\n",
        "        for line in fin:\n",
        "            line = line.rstrip('\\n')\n",
        "            line = [encoding[c] for c in line]\n",
        "            seed.append(np.array(line[:]).T)\n",
        "         \n",
        "    \n",
        "    calibrationset = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(np.stack(seed), dtype = torch.float32))\n",
        "    loader = torch.utils.data.DataLoader(calibrationset, batch_size=10000, shuffle=False, num_workers=2, drop_last=False)\n",
        "    \n",
        "    outputs = []\n",
        "    seedModel, suffixModel, combine = model\n",
        "    with torch.no_grad():\n",
        "        for seed in tqdm(loader, position = 0, leave = True):\n",
        "            outputs.append( combine( seedModel(seed.cuda())))\n",
        "    return np.std(np.concatenate(outputs), ddof = 1)\n",
        "\n",
        "def getTranslate():\n",
        "    with open(\"../model_training/encoding.pkl\", 'rb') as fin:\n",
        "        onehot = pickle.load(fin)\n",
        "    dic = {}\n",
        "    for k in onehot:\n",
        "        dic[k] = torch.tensor(onehot[k]).float().cuda()\n",
        "    return dic\n"
      ],
      "metadata": {
        "id": "Tm8QlOT53Ng_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objects for library generation"
      ],
      "metadata": {
        "id": "8BbVIljw8E9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class runningSum():\n",
        "    def __init__(self, embedShape):\n",
        "        self.x = torch.zeros(*embedShape).double()\n",
        "        self._c = torch.zeros(*embedShape).double()\n",
        "        \n",
        "    def add(self, y):\n",
        "        self.x = self.x + y\n",
        "\n",
        "class Translator():\n",
        "    def __init__(self, translate, model):\n",
        "        self.SymbolToCode = translate\n",
        "        self.NumberToLetter = [aa for aa in self.SymbolToCode]\n",
        "        self.NumberToCode = []\n",
        "        self.SymbolToNumber = {}\n",
        "        self.CodeToEmbedding = model\n",
        "        self.pad = -1\n",
        "        for i, aa in enumerate(self.NumberToLetter):\n",
        "            self.SymbolToNumber[aa] = i\n",
        "            self.NumberToCode.append( self.SymbolToCode[aa] )\n",
        "            if aa == 'J':\n",
        "                self.pad = i\n",
        "            \n",
        "    def toString(self, seq):\n",
        "        return ''.join([self.NumberToLetter[i] for i in seq])\n",
        "\n",
        "class Sequence():\n",
        "    def __init__(self, sequence, translator):\n",
        "        self.translator = translator\n",
        "        self.sequence = [self.translator.SymbolToNumber[c] for c in sequence]\n",
        "        self.code = torch.stack( [self.translator.NumberToCode[c] for c in self.sequence] )\n",
        "        self.tracker = None\n",
        "        self.runningSum = None\n",
        "        self.embedding = None\n",
        "            \n",
        "    def register(self, tracker):\n",
        "        self.tracker = tracker\n",
        "        for i,j in enumerate(self.sequence):\n",
        "            self.tracker.counts[i][j] += 1\n",
        "        self.tracker.seqs.add( self.translator.toString(self.sequence) )\n",
        "        \n",
        "    def register2(self, runningSum):\n",
        "        self.runningSum = runningSum\n",
        "        self.runningSum.add(self.embedding)\n",
        "        \n",
        "    def update(self, position, aa_number, new_embedding):\n",
        "        self.tracker.seqs.remove( self.translator.toString(self.sequence) )\n",
        "        self.tracker.counts[position][ self.sequence[position] ] -= 1\n",
        "        self.sequence[position] = aa_number\n",
        "        self.tracker.counts[position][ aa_number ] += 1\n",
        "        self.tracker.seqs.add( self.translator.toString(self.sequence) )\n",
        "        \n",
        "        self.code[position] = self.translator.NumberToCode[aa_number]\n",
        "        with torch.no_grad():\n",
        "            self.runningSum.add(-self.embedding.double())\n",
        "            self.embedding = new_embedding\n",
        "            self.runningSum.add(self.embedding.double())\n",
        "            \n",
        "    def isFree(self, position, proposals):\n",
        "        current = self.sequence[position]\n",
        "        freeMove = []\n",
        "        for i in proposals:\n",
        "            self.sequence[position] = i\n",
        "            freeMove.append(self.translator.toString(self.sequence) not in self.tracker.seqs)\n",
        "        self.sequence[position] = current\n",
        "        return freeMove\n",
        "    \n",
        "    # Generate a list of embeddings corresponding to samples that can then be used to calculate deltas upstream\n",
        "    def mockUpdates(self, position):\n",
        "        samples = []\n",
        "        current = self.sequence[position]\n",
        "        for i in range(len(self.translator.NumberToCode)):\n",
        "            if i != current and i != self.translator.pad:\n",
        "                self.sequence[position] = i\n",
        "                samples.append(i)\n",
        "        self.sequence[position] = current\n",
        "        if len(samples) == 0:\n",
        "            return [], []\n",
        "        \n",
        "        seqs = torch.stack( [self.code for i in range(len(samples))] )\n",
        "        for i,proposal in enumerate(samples):\n",
        "            seqs[i, position] = self.translator.NumberToCode[proposal]\n",
        "            \n",
        "        return seqs, samples\n",
        "\n",
        "class FrequencyTracker():\n",
        "    def __init__(self, sequences, length, sigma):\n",
        "        self.counts = np.zeros( (length, sigma) )\n",
        "        self.seqs = set()\n",
        "        for seq in sequences:\n",
        "            seq.register(self)\n",
        "        \n",
        "class SubLibrary():\n",
        "    def __init__(self, sequences, translate, model, updateParam, length):\n",
        "        sigma = len(translate)\n",
        "        \n",
        "        self.translator = Translator(translate, model)\n",
        "        self.seqs = [Sequence(seq, self.translator) for seq in sequences]\n",
        "        self.size = len(self.seqs)\n",
        "        self.updateParam = updateParam\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            embeds = self.translator.CodeToEmbedding( torch.stack([seq.code for seq in self.seqs]).permute(0,2,1) ).cpu()\n",
        "            for i, seq in enumerate(self.seqs):\n",
        "                seq.embedding = embeds[i].detach()\n",
        "        \n",
        "        partition = {}\n",
        "        for seq, update in zip(self.seqs, self.updateParam):\n",
        "            if update not in partition:\n",
        "                partition[update] = []\n",
        "            partition[update].append(seq)\n",
        "        \n",
        "        self.frequencyTrackers = {}\n",
        "        for k in partition:\n",
        "            self.frequencyTrackers[k] = FrequencyTracker(partition[k], length, sigma)\n",
        "        \n",
        "        self.embeddingSum = runningSum(self.seqs[0].embedding.shape)\n",
        "        for seq in self.seqs:\n",
        "            seq.register2(self.embeddingSum)\n",
        "            \n",
        "    def _entropy(self, arr, z):\n",
        "        return -np.log(np.maximum(1,arr)/z) * (arr/z)\n",
        "    \n",
        "    def dedupe(self, seqindex, position, ls):\n",
        "        ls2 = []\n",
        "        proposals = ls[0]\n",
        "        isFree = self.seqs[seqindex].isFree(position, proposals)\n",
        "        for arr in ls:\n",
        "            ls2.append([z for z,x in zip(arr,isFree) if x])\n",
        "        return ls2\n",
        "    \n",
        "    def getEntropyChanges(self, counts, current, proposals):\n",
        "        x = counts[current]\n",
        "        z = np.sum(counts)\n",
        "        h_now = self._entropy(counts, z)\n",
        "        dh = self._entropy(counts[current]-1, z) - h_now[current]\n",
        "        deltah = (self._entropy(counts+1, z) - h_now) + dh\n",
        "        return np.take(deltah, proposals) * z\n",
        "    \n",
        "    def proposeEveryChange(self, seqindex, position):\n",
        "        codes, proposals = self.seqs[seqindex].mockUpdates(position)\n",
        "        if len(proposals) == 0: return [], None, []\n",
        "        return codes, self.seqs[seqindex].embedding, proposals\n",
        "    \n",
        "    def getDeltaEntropy(self, seqindex, position, proposals):\n",
        "        deltaEntropy = self.getEntropyChanges(self.seqs[seqindex].tracker.counts[position],\n",
        "                                              self.seqs[seqindex].sequence[position],\n",
        "                                              proposals)\n",
        "        return deltaEntropy\n",
        "    \n",
        "    def update(self, seqindex, position, aa_number, new_embedding):\n",
        "        self.seqs[seqindex].update(position, aa_number, new_embedding)\n",
        "        \n",
        "    def getEntropyScores(self):\n",
        "        report = []\n",
        "        for k in self.frequencyTrackers:\n",
        "            counts = self.frequencyTrackers[k].counts\n",
        "            z = np.sum(counts[0])\n",
        "            h = np.array( [np.sum( self._entropy(arr, z) ) for arr in counts] )\n",
        "            report.append( (z,h) )\n",
        "        return report\n",
        "    \n",
        "    def getLib(self):\n",
        "        return [self.translator.toString(z.sequence) for z in self.seqs]\n",
        "\n",
        "class Optimizer():\n",
        "    def __init__(self, s_seed, suffix_seed, updateSeed, seedLength, translate, model, std):\n",
        "        seedModel, suffixModel, combine = model\n",
        "        self.seedLib = SubLibrary(s_seed, translate, seedModel, updateSeed, seedLength)\n",
        "        self.combine = combine\n",
        "        self.entropyWeight = 0.01\n",
        "        self.T = 50\n",
        "        self.std = std\n",
        "        self.updateSeed = updateSeed\n",
        "        self.seedLength = seedLength\n",
        "        \n",
        "    def sweep(self):\n",
        "        sweep = [i for i in range(self.seedLength)]\n",
        "        for s in zip(sweep):\n",
        "            scol = self.sweepColumn(p, self.updateSeed, self.seedLib)\n",
        "            for i in range(max(len(pcol), len(scol))):\n",
        "                if i < len(pcol):\n",
        "                    self.executeUpdate(True, pcol[i], self.seedLib, s)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "                    \n",
        "    def executeUpdate(self, isSeed, command, lib, otherlib, position):\n",
        "        seqindex, dEmbed, cEmbed, proposals = command\n",
        "        with torch.no_grad():\n",
        "            dScore = (self.combine.getSeed(dEmbed-cEmbed, self.suffixLib.embeddingSum.x.float()) if isSeed\n",
        "                      else self.combine.getSuffix(self.seedLib.embeddingSum.x.float(), dEmbed-cEmbed)).numpy()\n",
        "            \n",
        "        proposals, dEmbed, dScore = lib.dedupe( seqindex, position, (proposals, dEmbed, dScore) )\n",
        "        dScore = np.array(dScore)\n",
        "        if len(proposals) == 0: return\n",
        "        \n",
        "        dEntropy = lib.getDeltaEntropy(seqindex, position, proposals)\n",
        "        dScore = (dScore/ (otherlib.size*self.std)) + (dEntropy * self.entropyWeight)\n",
        "        \n",
        "        dScore = np.concatenate( ((dScore * self.T), [0]) )\n",
        "        proposals.append(-1)\n",
        "        newIndex = random.choices(range(len(proposals)), np.exp(dScore - np.max(dScore)))[0]\n",
        "        if proposals[newIndex] != -1:\n",
        "            lib.update(seqindex, position, proposals[newIndex], dEmbed[newIndex].detach())\n",
        "          \n",
        "    def sweepColumn(self, position, updates, lib):\n",
        "        index = 0\n",
        "        commands = []\n",
        "        bigTensor = []\n",
        "        for seqindex, (mn, mx) in enumerate( updates ):\n",
        "            if mn <= position < mx:\n",
        "                codeProposals, cEmbed, proposals = lib.proposeEveryChange(seqindex, position)\n",
        "                if len(proposals) == 0: continue\n",
        "                \n",
        "                rng = (index, index + len(proposals))\n",
        "                commands.append( (seqindex, cEmbed, proposals, rng) )\n",
        "                bigTensor.append(codeProposals)\n",
        "                index += len(proposals)\n",
        "        if len(bigTensor) == 0: return []\n",
        "        with torch.no_grad():\n",
        "            outTensor = lib.translator.CodeToEmbedding( torch.cat(bigTensor, dim = 0).permute((0,2,1)).cuda() )\n",
        "        commands2 = []\n",
        "        for i, (seqindex, cEmbed, proposals, rng) in enumerate(commands):\n",
        "            dEmbed = outTensor[rng[0]:rng[1]].detach().cpu()\n",
        "            commands2.append((seqindex, dEmbed, cEmbed, proposals))\n",
        "        return commands2\n",
        "        \n",
        "    def getScore(self):\n",
        "        with torch.no_grad():\n",
        "            score = self.combine(self.seedLib.embeddingSum.x.float().unsqueeze(0))\n",
        "        h = self.seedLib.getEntropyScores()\n",
        "        ht = np.sum([z*h for z,h in h2]) * self.seedLib.size\n",
        "        total = -score + (ht*self.entropyWeight)\n",
        "        return total\n",
        "    \n",
        "    def getLib(self):\n",
        "        return self.seedLib.getLib()"
      ],
      "metadata": {
        "id": "MzoVGk0B34g9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSeed(n):\n",
        "    aas = list(\"ACDEFGHIKLMNPQRSTVWYJ\")\n",
        "    sample = np.random.randint(0,20, (n*2, 10) )\n",
        "    sample = set(sample)\n",
        "    if len(sample) < n:\n",
        "        return getSeed(n)\n",
        "    return list(sample)[:n]\n"
      ],
      "metadata": {
        "id": "I6yo4LgY5oiQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a factorizable library"
      ],
      "metadata": {
        "id": "i89HW0SA53Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ensemble with a list of models\n",
        "# Here, you can change the path to a folder containing model weights of your choice\n",
        "# In this example, we optimize ranibizumab affinity\n",
        "model_dir = \"../model_training/trained_weights/reverse_kernel/baculovirus/\"\n",
        "ensembles = loadModels([os.path.join(model_dir, file) for file in os.listdir(model_dir)])\n",
        "\n",
        "# Determine the standard deviation of the ensemble\n",
        "std = calibrate(ensembles)"
      ],
      "metadata": {
        "id": "dP6f9GN057Hk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "f37350f8-afdb-4853-fb79-833b25cce3a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-078644a57327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# In this example, we optimize ranibizumab affinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../model_training/trained_weights/reverse_kernel/baculovirus/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mensembles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Determine the standard deviation of the ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-15417109b932>\u001b[0m in \u001b[0;36mloadModels\u001b[0;34m(fns)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor_Joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mseedMaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedSeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mseedEnsemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseedMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Predictor_Joint:\n\tMissing key(s) in state_dict: \"embedSeed.position\", \"embedSeed.conv.weight\", \"embedSeed.conv.bias\", \"embedSeed.conv_bn.weight\", \"embedSeed.conv_bn.bias\", \"embedSeed.conv_bn.running_mean\", \"embedSeed.conv_bn.running_var\", \"embedSeed.block1.conv1.weight\", \"embedSeed.block1.conv1.bias\", \"embedSeed.block1.conv1_bn.weight\", \"embedSeed.block1.conv1_bn.bias\", \"embedSeed.block1.conv1_bn.running_mean\", \"embedSeed.block1.conv1_bn.running_var\", \"embedSeed.block1.conv2.weight\", \"embedSeed.block1.conv2.bias\", \"embedSeed.block1.conv2_bn.weight\", \"embedSeed.block1.conv2_bn.bias\", \"embedSeed.block1.conv2_bn.running_mean\", \"embedSeed.block1.conv2_bn.running_var\", \"embedSeed.block2.conv1.weight\", \"embedSeed.block2.conv1.bias\", \"embedSeed.block2.conv1_bn.weight\", \"embedSeed.block2.conv1_bn.bias\", \"embedSeed.block2.conv1_bn.running_mean\", \"embedSeed.block2.conv1_bn.running_var\", \"embedSeed.block2.conv2.weight\", \"embedSeed.block2.conv2.bias\", \"embedSeed.block2.conv2_bn.weight\", \"embedSeed.block2.conv2_bn.bias\", \"embedSeed.block2.conv2_bn.running_mean\", \"embedSeed.block2.conv2_bn.running_var\", \"embedSeed.block3.conv1.weight\", \"embedSeed.block3.conv1.bias\", \"embedSeed.block3.conv1_bn.weight\", \"embedSeed.block3.conv1_bn.bias\", \"embedSeed.block3.conv1_bn.running_mean\", \"embedSeed.block3.conv1_bn.running_var\", \"embedSeed.block3.conv2.weight\", \"embedSeed.block3.conv2.bias\", \"embedSeed.block3.conv2_bn.weight\", \"embedSeed.block3.conv2_bn.bias\", \"embedSeed.block3.conv2_bn.running_mean\", \"embedSeed.blo...\n\tUnexpected key(s) in state_dict: \"embedPrefix.position\", \"embedPrefix.conv.weight\", \"embedPrefix.conv.bias\", \"embedPrefix.conv_bn.weight\", \"embedPrefix.conv_bn.bias\", \"embedPrefix.conv_bn.running_mean\", \"embedPrefix.conv_bn.running_var\", \"embedPrefix.conv_bn.num_batches_tracked\", \"embedPrefix.block1.conv1.weight\", \"embedPrefix.block1.conv1.bias\", \"embedPrefix.block1.conv1_bn.weight\", \"embedPrefix.block1.conv1_bn.bias\", \"embedPrefix.block1.conv1_bn.running_mean\", \"embedPrefix.block1.conv1_bn.running_var\", \"embedPrefix.block1.conv1_bn.num_batches_tracked\", \"embedPrefix.block1.conv2.weight\", \"embedPrefix.block1.conv2.bias\", \"embedPrefix.block1.conv2_bn.weight\", \"embedPrefix.block1.conv2_bn.bias\", \"embedPrefix.block1.conv2_bn.running_mean\", \"embedPrefix.block1.conv2_bn.running_var\", \"embedPrefix.block1.conv2_bn.num_batches_tracked\", \"embedPrefix.block2.conv1.weight\", \"embedPrefix.block2.conv1.bias\", \"embedPrefix.block2.conv1_bn.weight\", \"embedPrefix.block2.conv1_bn.bias\", \"embedPrefix.block2.conv1_bn.running_mean\", \"embedPrefix.block2.conv1_bn.running_var\", \"embedPrefix.block2.conv1_bn.num_batches_tracked\", \"embedPrefix.block2.conv2.weight\", \"embedPrefix.block2.conv2.bias\", \"embedPrefix.block2.conv2_bn.weight\", \"embedPrefix.block2.conv2_bn.bias\", \"embedPrefix.block2.conv2_bn.running_mean\", \"embedPrefix.block2.conv2_bn.running_var\", \"embedPrefix.block2.conv2_bn.num_batches_tracked\", \"embedPrefix.block3.conv1.weight\", \"embedPrefix.block3.conv1.bias\", \"embedPrefix.block3.conv1_b..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize seed sequences\n",
        "p,pu = getSeed(10)\n",
        "\n",
        "\n",
        "# Initialize the optimizer\n",
        "opt = Optimizer(s_seed=p, \n",
        "                updateSeed=pu,  \n",
        "                seedLength=10,  \n",
        "                translate=getTranslate(), \n",
        "                model=ensembles, \n",
        "                std=std)\n",
        "\n",
        "# Set entropy parameter lambda\n",
        "opt.entropyWeight = 0.1"
      ],
      "metadata": {
        "id": "zVXgNVqx5-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize temperature\n",
        "opt.T = 1\n",
        "\n",
        "bestLib = None\n",
        "bestScore = float(\"-inf\")\n",
        "\n",
        "# Optimize for 10 iterations\n",
        "# change the value in the range() function to increase iterations\n",
        "pbar = tqdm(range(10), position=0, leave=True)\n",
        "for i in pbar:\n",
        "    opt.sweep()\n",
        "    torch.cuda.empty_cache()\n",
        "    score = opt.getScore()\n",
        "    if score > bestScore:\n",
        "        bestLib = opt.getLib()\n",
        "    pbar.set_description(\"Score = {:.2f}, Temperature = {:.5f}\".format(score, 1/opt.T))\n",
        "    \n",
        "    # Lower the temperature every 5 iterations\n",
        "    if i%5==4:\n",
        "        opt.T *= 1.1\n",
        "        \n",
        "# Optimized segment libraries\n",
        "seedLibrary = bestLib"
      ],
      "metadata": {
        "id": "EOQqFIPE6jlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be07e75f-f4b5-491d-aa53-da0a087a66fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Score = 15830.26, Temperature = 0.90909: 100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp = pprint.PrettyPrinter(width=150, compact=True)\n",
        "print(\"Baculovirus library: \\n\")\n",
        "pp.pprint(seedLibrary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE_A2li7MlfG",
        "outputId": "84058ad3-e081-4783-9e67-fa100ba5c102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prefix library: \n",
            "\n",
            "['CRQRNCGDPG', 'WIPRIRLWWG', 'SPRSKCRCKA', 'GDQWPWKMRR', 'YFSIWTECDI', 'CAHCRCRHLW', 'GRGLDFQEQA', 'WDYSWGVNPA', 'KYFKCMQKIA', 'LMCQNGCLMH',\n",
            " 'JFYRLWCVSP', 'JTSQWICGKN', 'JFCHNTYMCH', 'JWQMLQQNSS', 'JLVRNWKCGC', 'JCKQKNHHRE', 'JCQQEYCEFV', 'JKNRNQQPHW', 'JGFWEAYRSW', 'JLIRLQRIKL',\n",
            " 'JJSSSTWMNW', 'JJMKKCNIRP', 'JJSGHRPQLD', 'JJLIISTMNS', 'JJFCFRKAFM', 'JJCRRQLPMY', 'JJERMIIIKV', 'JJKLKFVKMV', 'JJHGETHFPL', 'JJWHLSRCYK',\n",
            " 'JJJIKSRKTN', 'JJJVFYYMWC', 'JJJACKNKLP', 'JJJWKNIMTK', 'JJJTCVVIDA', 'JJJLKNKVGN', 'JJJQRRQVNK', 'JJJYHGLCNK', 'JJJRSQKPMT', 'JJJARSRVSF',\n",
            " 'JJJJMFNAWW', 'JJJJWCQQSF', 'JJJJCCCQCQ', 'JJJJTTFVLI', 'JJJJSKKHNP', 'JJJJEFELSC', 'JJJJPWWADF', 'JJJJINITFR', 'JJJJWLCTKH', 'JJJJWCPSAR',\n",
            " 'JJJJJWNLWW', 'JJJJJQQYKD', 'JJJJJYKCFL', 'JJJJJDNVKY', 'JJJJJFKNIC', 'JJJJJVPDTK', 'JJJJJLEERC', 'JJJJJFRARV', 'JJJJJILTCF', 'JJJJJYCRHE',\n",
            " 'JJJJJJVWDS', 'JJJJJJRKNI', 'JJJJJJQNGN', 'JJJJJJRCKV', 'JJJJJJWWCA', 'JJJJJJNYIG', 'JJJJJJCVYA', 'JJJJJJLCKN', 'JJJJJJMCTA', 'JJJJJJWLPI']\n",
            "Suffix library: \n",
            "\n",
            "['VKNMVPQWHL', 'TVHYDVYRHI', 'PFKEDGGGNT', 'TFDRAMFVTR', 'IPNHYGIQSC', 'ALSLTRDKRE', 'RTNSMQQRIY', 'KFENNECTFA', 'TCATGHMEYA', 'VPWMIPVCGC',\n",
            " 'WLIPVNHEGJ', 'CFRFDRHKNJ', 'LCKKTKGNNJ', 'NMHTAHANSJ', 'HFLYHGDCYJ', 'CLTSPKKFIJ', 'GLTRCHDSHJ', 'YVKLYYKLDJ', 'YRTRNWCWMJ', 'MNGKDGYVGJ',\n",
            " 'PSLPHASIJJ', 'KWRHTLSCJJ', 'RLNFMFYAJJ', 'AWPGLRCRJJ', 'NCDVGMVAJJ', 'TCEPTQEQJJ', 'WRQDWNFYJJ', 'WKTSQKYLJJ', 'MMCNMTGEJJ', 'GYKFKDWTJJ',\n",
            " 'VKRILCYJJJ', 'KKSIETRJJJ', 'KQIMYMKJJJ', 'WIGHWHGJJJ', 'NHLCGCNJJJ', 'MRFSNFKJJJ', 'CVTCWACJJJ', 'LKQTRYNJJJ', 'MWCHQQAJJJ', 'QAQNRHRJJJ',\n",
            " 'QSGCNRJJJJ', 'SLTDYFJJJJ', 'EDTHVAJJJJ', 'DCWAQHJJJJ', 'MNYIASJJJJ', 'AWRKHAJJJJ', 'IQEQFTJJJJ', 'ERKFLIJJJJ', 'EDMKFIJJJJ', 'RHAQRDJJJJ',\n",
            " 'RRAVDJJJJJ', 'WEFNAJJJJJ', 'VWGYNJJJJJ', 'ACWCSJJJJJ', 'TPVAIJJJJJ', 'SCHINJJJJJ', 'MSYHMJJJJJ', 'TVFKIJJJJJ', 'MMHLLJJJJJ', 'WYMWEJJJJJ',\n",
            " 'NNECJJJJJJ', 'IGAKJJJJJJ', 'WHIPJJJJJJ', 'FLLKJJJJJJ', 'IGHTJJJJJJ', 'NFKRJJJJJJ', 'HCILJJJJJJ', 'YIGFJJJJJJ', 'PINVJJJJJJ', 'LTNWJJJJJJ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "po-65ferM552"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}