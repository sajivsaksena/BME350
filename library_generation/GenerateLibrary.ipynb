{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, dilation):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
    "        self.conv1_bn = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
    "        self.conv2_bn = nn.BatchNorm1d(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu( self.conv1_bn( self.conv1(x) ) )\n",
    "        return self.conv2_bn( self.conv2(x1) ) + x\n",
    "\n",
    "class EmbedderNet(nn.Module):\n",
    "    def __init__(self, length, channels, outchannels):\n",
    "        super(EmbedderNet, self).__init__()\n",
    "        self.length = length\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(2, 2, ceil_mode = True)\n",
    "        self.conv = nn.Conv1d(40, channels, 1, 1, 0)\n",
    "        self.conv_bn = nn.BatchNorm1d(channels)\n",
    "        \n",
    "        self.block1 = ResBlock(channels, 1)\n",
    "        self.block2 = ResBlock(channels, 1)\n",
    "        self.block3 = ResBlock(channels, 1)\n",
    "        self.block4 = ResBlock(channels, 1)\n",
    "        self.block5 = ResBlock(channels, 1)\n",
    "        \n",
    "        self.embed_1 = nn.Linear( int( channels*math.ceil(math.ceil(self.length/2)/2) ) , 128)\n",
    "        self.embed_2 = nn.Linear(128, outchannels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        x = F.relu( self.conv_bn( self.conv(x) ) )\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool( self.block4(x) )\n",
    "        x = self.pool( self.block5(x) ).view(batchsize,-1)\n",
    "        return self.embed_2( F.relu( self.embed_1(x) ) )\n",
    "    \n",
    "class Predictor_Dot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor_Dot, self).__init__()\n",
    "        self.embedPrefix = EmbedderNet(10, 64, 16)\n",
    "        self.embedSuffix = EmbedderNet(10, 64, 16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        prefix = x[:,:,:10]\n",
    "        suffix = x[:,:,10:]\n",
    "        return torch.sum(self.embedPrefix(prefix) * self.embedSuffix(suffix), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetEnsemble(nn.Module):\n",
    "    def __init__(self, lst):\n",
    "        super(NetEnsemble, self).__init__()\n",
    "        self.nets = nn.ModuleList(lst)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.stack([net(x) for net in self.nets]).permute((1,0,2))\n",
    "\n",
    "class Combine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Combine, self).__init__()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        return torch.einsum(\"nij,nij->n\",x1,x2)\n",
    "        \n",
    "    def getPrefix(self, x1, x2):\n",
    "        return torch.einsum(\"nij,ij->n\",x1,x2)\n",
    "    \n",
    "    def getSuffix(self, x1, x2):\n",
    "        return torch.einsum(\"ij,nij->n\",x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in a list of models to get an ensemble\n",
    "def loadModels(fns):\n",
    "    prefixMaps = []\n",
    "    suffixMaps = []\n",
    "    for fn in fns:\n",
    "        net = Predictor_Dot()\n",
    "        net.load_state_dict(torch.load( fn, map_location='cpu' ))\n",
    "        prefixMaps.append(net.embedPrefix)\n",
    "        suffixMaps.append(net.embedSuffix)\n",
    "    prefixEnsemble = NetEnsemble(prefixMaps)\n",
    "    suffixEnsemble = NetEnsemble(suffixMaps)\n",
    "    combine = Combine()\n",
    "    prefixEnsemble.eval().cuda()\n",
    "    suffixEnsemble.eval().cuda()\n",
    "    combine.eval()\n",
    "    return prefixEnsemble, suffixEnsemble, combine\n",
    "\n",
    "def calibrate(model):\n",
    "    with open(\"../model_training/encoding.pkl\", 'rb') as fin:\n",
    "        encoding = pickle.load(fin)\n",
    "    prefix = []\n",
    "    suffix = []\n",
    "    with open(\"calibration.txt\", 'rt') as fin:\n",
    "        for line in fin:\n",
    "            line = line.rstrip('\\n')\n",
    "            line = [encoding[c] for c in line]\n",
    "            prefix.append(np.array(line[:10]).T)\n",
    "            suffix.append(np.array(line[10:]).T)\n",
    "    \n",
    "    calibrationset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(np.stack(prefix), dtype = torch.float32),\n",
    "        torch.tensor(np.stack(suffix), dtype = torch.float32))\n",
    "    loader = torch.utils.data.DataLoader(calibrationset, batch_size=10000, shuffle=False, num_workers=2, drop_last=False)\n",
    "    \n",
    "    outputs = []\n",
    "    prefixModel, suffixModel, combine = model\n",
    "    with torch.no_grad():\n",
    "        for prefix, suffix in tqdm(loader, position = 0, leave = True):\n",
    "            outputs.append( combine( prefixModel(prefix.cuda()), suffixModel(suffix.cuda()) ).cpu().numpy() )\n",
    "    return np.std(np.concatenate(outputs), ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTranslate():\n",
    "    with open(\"../model_training/encoding.pkl\", 'rb') as fin:\n",
    "        onehot = pickle.load(fin)\n",
    "    dic = {}\n",
    "    for k in onehot:\n",
    "        dic[k] = torch.tensor(onehot[k]).float().cuda()\n",
    "    return dic\n",
    "\n",
    "class runningSum():\n",
    "    def __init__(self, embedShape):\n",
    "        self.x = torch.zeros(*embedShape).double()\n",
    "        self._c = torch.zeros(*embedShape).double()\n",
    "        \n",
    "    def add(self, y):\n",
    "        self.x = self.x + y\n",
    "\n",
    "class Translator():\n",
    "    def __init__(self, translate, model):\n",
    "        self.SymbolToCode = translate\n",
    "        self.NumberToLetter = [aa for aa in self.SymbolToCode]\n",
    "        self.NumberToCode = []\n",
    "        self.SymbolToNumber = {}\n",
    "        self.CodeToEmbedding = model\n",
    "        self.pad = -1\n",
    "        for i, aa in enumerate(self.NumberToLetter):\n",
    "            self.SymbolToNumber[aa] = i\n",
    "            self.NumberToCode.append( self.SymbolToCode[aa] )\n",
    "            if aa == 'J':\n",
    "                self.pad = i\n",
    "            \n",
    "    def toString(self, seq):\n",
    "        return ''.join([self.NumberToLetter[i] for i in seq])\n",
    "\n",
    "class Sequence():\n",
    "    def __init__(self, sequence, translator):\n",
    "        self.translator = translator\n",
    "        self.sequence = [self.translator.SymbolToNumber[c] for c in sequence]\n",
    "        self.code = torch.stack( [self.translator.NumberToCode[c] for c in self.sequence] )\n",
    "        self.tracker = None\n",
    "        self.runningSum = None\n",
    "        self.embedding = None\n",
    "            \n",
    "    def register(self, tracker):\n",
    "        self.tracker = tracker\n",
    "        for i,j in enumerate(self.sequence):\n",
    "            self.tracker.counts[i][j] += 1\n",
    "        self.tracker.seqs.add( self.translator.toString(self.sequence) )\n",
    "        \n",
    "    def register2(self, runningSum):\n",
    "        self.runningSum = runningSum\n",
    "        self.runningSum.add(self.embedding)\n",
    "        \n",
    "    def update(self, position, aa_number, new_embedding):\n",
    "        self.tracker.seqs.remove( self.translator.toString(self.sequence) )\n",
    "        self.tracker.counts[position][ self.sequence[position] ] -= 1\n",
    "        self.sequence[position] = aa_number\n",
    "        self.tracker.counts[position][ aa_number ] += 1\n",
    "        self.tracker.seqs.add( self.translator.toString(self.sequence) )\n",
    "        \n",
    "        self.code[position] = self.translator.NumberToCode[aa_number]\n",
    "        with torch.no_grad():\n",
    "            self.runningSum.add(-self.embedding.double())\n",
    "            self.embedding = new_embedding\n",
    "            self.runningSum.add(self.embedding.double())\n",
    "            \n",
    "    def isFree(self, position, proposals):\n",
    "        current = self.sequence[position]\n",
    "        freeMove = []\n",
    "        for i in proposals:\n",
    "            self.sequence[position] = i\n",
    "            freeMove.append(self.translator.toString(self.sequence) not in self.tracker.seqs)\n",
    "        self.sequence[position] = current\n",
    "        return freeMove\n",
    "            \n",
    "    # Generate a list of embeddings corresponding to samples that can then be used to calculate deltas upstream\n",
    "    def mockUpdates(self, position):\n",
    "        samples = []\n",
    "        current = self.sequence[position]\n",
    "        for i in range(len(self.translator.NumberToCode)):\n",
    "            if i != current and i != self.translator.pad:\n",
    "                self.sequence[position] = i\n",
    "                samples.append(i)\n",
    "        self.sequence[position] = current\n",
    "        if len(samples) == 0:\n",
    "            return [], []\n",
    "        \n",
    "        seqs = torch.stack( [self.code for i in range(len(samples))] )\n",
    "        for i,proposal in enumerate(samples):\n",
    "            seqs[i, position] = self.translator.NumberToCode[proposal]\n",
    "            \n",
    "        return seqs, samples  \n",
    "        \n",
    "class FrequencyTracker():\n",
    "    def __init__(self, sequences, length, sigma):\n",
    "        self.counts = np.zeros( (length, sigma) )\n",
    "        self.seqs = set()\n",
    "        for seq in sequences:\n",
    "            seq.register(self)\n",
    "        \n",
    "class SubLibrary():\n",
    "    def __init__(self, sequences, translate, model, updateParam, length):\n",
    "        sigma = len(translate)\n",
    "        \n",
    "        self.translator = Translator(translate, model)\n",
    "        self.seqs = [Sequence(seq, self.translator) for seq in sequences]\n",
    "        self.size = len(self.seqs)\n",
    "        self.updateParam = updateParam\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeds = self.translator.CodeToEmbedding( torch.stack([seq.code for seq in self.seqs]).permute(0,2,1) ).cpu()\n",
    "            for i, seq in enumerate(self.seqs):\n",
    "                seq.embedding = embeds[i].detach()\n",
    "        \n",
    "        partition = {}\n",
    "        for seq, update in zip(self.seqs, self.updateParam):\n",
    "            if update not in partition:\n",
    "                partition[update] = []\n",
    "            partition[update].append(seq)\n",
    "        \n",
    "        self.frequencyTrackers = {}\n",
    "        for k in partition:\n",
    "            self.frequencyTrackers[k] = FrequencyTracker(partition[k], length, sigma)\n",
    "        \n",
    "        self.embeddingSum = runningSum(self.seqs[0].embedding.shape)\n",
    "        for seq in self.seqs:\n",
    "            seq.register2(self.embeddingSum)\n",
    "            \n",
    "    def _entropy(self, arr, z):\n",
    "        return -np.log(np.maximum(1,arr)/z) * (arr/z)\n",
    "    \n",
    "    def dedupe(self, seqindex, position, ls):\n",
    "        ls2 = []\n",
    "        proposals = ls[0]\n",
    "        isFree = self.seqs[seqindex].isFree(position, proposals)\n",
    "        for arr in ls:\n",
    "            ls2.append([z for z,x in zip(arr,isFree) if x])\n",
    "        return ls2\n",
    "    \n",
    "    def getEntropyChanges(self, counts, current, proposals):\n",
    "        x = counts[current]\n",
    "        z = np.sum(counts)\n",
    "        h_now = self._entropy(counts, z)\n",
    "        dh = self._entropy(counts[current]-1, z) - h_now[current]\n",
    "        deltah = (self._entropy(counts+1, z) - h_now) + dh\n",
    "        return np.take(deltah, proposals) * z\n",
    "    \n",
    "    def proposeEveryChange(self, seqindex, position):\n",
    "        codes, proposals = self.seqs[seqindex].mockUpdates(position)\n",
    "        if len(proposals) == 0: return [], None, []\n",
    "        return codes, self.seqs[seqindex].embedding, proposals\n",
    "    \n",
    "    def getDeltaEntropy(self, seqindex, position, proposals):\n",
    "        deltaEntropy = self.getEntropyChanges(self.seqs[seqindex].tracker.counts[position],\n",
    "                                              self.seqs[seqindex].sequence[position],\n",
    "                                              proposals)\n",
    "        return deltaEntropy\n",
    "    \n",
    "    def update(self, seqindex, position, aa_number, new_embedding):\n",
    "        self.seqs[seqindex].update(position, aa_number, new_embedding)\n",
    "        \n",
    "    def getEntropyScores(self):\n",
    "        report = []\n",
    "        for k in self.frequencyTrackers:\n",
    "            counts = self.frequencyTrackers[k].counts\n",
    "            z = np.sum(counts[0])\n",
    "            h = np.array( [np.sum( self._entropy(arr, z) ) for arr in counts] )\n",
    "            report.append( (z,h) )\n",
    "        return report\n",
    "    \n",
    "    def getLib(self):\n",
    "        return [self.translator.toString(z.sequence) for z in self.seqs]\n",
    "\n",
    "class Optimizer():\n",
    "    def __init__(self, prefix_seed, suffix_seed, updatePrefix, updateSuffix, prefixLength, suffixLength, translate, model, std):\n",
    "        prefixModel, suffixModel, combine = model\n",
    "        self.prefixLib = SubLibrary(prefix_seed, translate, prefixModel, updatePrefix, prefixLength)\n",
    "        self.suffixLib = SubLibrary(suffix_seed, translate, suffixModel, updateSuffix, suffixLength)\n",
    "        self.combine = combine\n",
    "        self.entropyWeight = 0.01\n",
    "        self.T = 50\n",
    "        self.std = std\n",
    "        self.updatePrefix = updatePrefix\n",
    "        self.updateSuffix = updateSuffix\n",
    "        self.prefixLength = prefixLength\n",
    "        self.suffixLength = suffixLength\n",
    "        \n",
    "    def sweep(self):\n",
    "        psweep = [i for i in range(self.prefixLength)]\n",
    "        ssweep = [i for i in range(self.suffixLength)[::-1]]\n",
    "        for p,s in zip(psweep, ssweep):\n",
    "            pcol = self.sweepColumn(p, self.updatePrefix, self.prefixLib)\n",
    "            scol = self.sweepColumn(s, self.updateSuffix, self.suffixLib)\n",
    "            for i in range(max(len(pcol), len(scol))):\n",
    "                if i < len(pcol):\n",
    "                    self.executeUpdate(True, pcol[i], self.prefixLib, self.suffixLib, p)\n",
    "                if i < len(scol):\n",
    "                    self.executeUpdate(False, scol[i], self.suffixLib, self.prefixLib, s)\n",
    "            pcol = []\n",
    "            scol = []\n",
    "            torch.cuda.empty_cache()\n",
    "                    \n",
    "    def executeUpdate(self, isPrefix, command, lib, otherlib, position):\n",
    "        seqindex, dEmbed, cEmbed, proposals = command\n",
    "        with torch.no_grad():\n",
    "            dScore = (self.combine.getPrefix(dEmbed-cEmbed, self.suffixLib.embeddingSum.x.float()) if isPrefix\n",
    "                      else self.combine.getSuffix(self.prefixLib.embeddingSum.x.float(), dEmbed-cEmbed)).numpy()\n",
    "            \n",
    "        proposals, dEmbed, dScore = lib.dedupe( seqindex, position, (proposals, dEmbed, dScore) )\n",
    "        dScore = np.array(dScore)\n",
    "        if len(proposals) == 0: return\n",
    "        \n",
    "        dEntropy = lib.getDeltaEntropy(seqindex, position, proposals)\n",
    "        dScore = (dScore/ (otherlib.size*self.std)) + (dEntropy * self.entropyWeight)\n",
    "        \n",
    "        dScore = np.concatenate( ((dScore * self.T), [0]) )\n",
    "        proposals.append(-1)\n",
    "        newIndex = random.choices(range(len(proposals)), np.exp(dScore - np.max(dScore)))[0]\n",
    "        if proposals[newIndex] != -1:\n",
    "            lib.update(seqindex, position, proposals[newIndex], dEmbed[newIndex].detach())\n",
    "            \n",
    "    def sweepColumn(self, position, updates, lib):\n",
    "        index = 0\n",
    "        commands = []\n",
    "        bigTensor = []\n",
    "        for seqindex, (mn, mx) in enumerate( updates ):\n",
    "            if mn <= position < mx:\n",
    "                codeProposals, cEmbed, proposals = lib.proposeEveryChange(seqindex, position)\n",
    "                if len(proposals) == 0: continue\n",
    "                \n",
    "                rng = (index, index + len(proposals))\n",
    "                commands.append( (seqindex, cEmbed, proposals, rng) )\n",
    "                bigTensor.append(codeProposals)\n",
    "                index += len(proposals)\n",
    "        if len(bigTensor) == 0: return []\n",
    "        with torch.no_grad():\n",
    "            outTensor = lib.translator.CodeToEmbedding( torch.cat(bigTensor, dim = 0).permute((0,2,1)).cuda() )\n",
    "        commands2 = []\n",
    "        for i, (seqindex, cEmbed, proposals, rng) in enumerate(commands):\n",
    "            dEmbed = outTensor[rng[0]:rng[1]].detach().cpu()\n",
    "            commands2.append((seqindex, dEmbed, cEmbed, proposals))\n",
    "        return commands2\n",
    "        \n",
    "    def getScore(self):\n",
    "        with torch.no_grad():\n",
    "            score = self.combine(self.prefixLib.embeddingSum.x.float().unsqueeze(0),\n",
    "                                 self.suffixLib.embeddingSum.x.float().unsqueeze(0)).cpu().view(-1).item()/self.std\n",
    "        h1 = self.prefixLib.getEntropyScores()\n",
    "        h2 = self.suffixLib.getEntropyScores()\n",
    "        ht1 = np.sum([z*h for z,h in h1]) * self.suffixLib.size\n",
    "        ht2 = np.sum([z*h for z,h in h2]) * self.prefixLib.size\n",
    "        ht = ht1 + ht2\n",
    "        total = score + (ht*self.entropyWeight)\n",
    "        return total\n",
    "    \n",
    "    def getLib(self):\n",
    "        return self.prefixLib.getLib(), self.suffixLib.getLib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeed(n,mn,mx):\n",
    "    aas = list(\"ACDEFGHIKLMNPQRSTVWYJ\")\n",
    "    sample = np.random.randint(0,20, (n*2, 10) )\n",
    "    sample[:,:mn] = 20\n",
    "    sample[:,10-mx:] = 20\n",
    "    sample = [''.join( [aas[i] for i in s] ) for s in sample]\n",
    "    sample = set(sample)\n",
    "    if len(sample) < n:\n",
    "        return getSeed(n,mn,mx)\n",
    "    return list(sample)[:n]\n",
    "    \n",
    "def getSeedPref(n):\n",
    "    seeds = []\n",
    "    updates = []\n",
    "    for i in range(0,7):\n",
    "        seeds += getSeed(n,i,0)\n",
    "        updates += [(i,10)]*n\n",
    "    return seeds, updates\n",
    "\n",
    "def getSeedSuff(n):\n",
    "    seeds = []\n",
    "    updates = []\n",
    "    for i in range(0,7):\n",
    "        seeds += getSeed(n,0,i)\n",
    "        updates += [(0,10-i)]*n\n",
    "    return seeds, updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating a factorizable library</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ensemble with a list of models\n",
    "ensembles = loadModels([\"../model_training/weights/<MODEL WEIGHT FILE HERE>\"])\n",
    "\n",
    "# Determine the standard deviation of the ensemble\n",
    "std = calibrate(ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seed sequences\n",
    "p,pu = getSeedPref(20)\n",
    "s,su = getSeedSuff(20)\n",
    "\n",
    "# Initialize the optimizer\n",
    "opt = Optimizer(p, s, pu, su, 10, 10, getTranslate(), ensembles, std)\n",
    "\n",
    "# Set entropy parameter lambda\n",
    "opt.entropyWeight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize temperature\n",
    "opt.T = 1\n",
    "\n",
    "bestLib = None\n",
    "bestScore = float(\"-inf\")\n",
    "\n",
    "# Optimize for 500 iterations\n",
    "pbar = tqdm(range(500), position=0, leave=True)\n",
    "for i in pbar:\n",
    "    opt.sweep()\n",
    "    torch.cuda.empty_cache()\n",
    "    score = opt.getScore()\n",
    "    if score > bestScore:\n",
    "        bestLib = opt.getLib()\n",
    "    pbar.set_description(\"Score = {:.2f}, Temperature = {:.5f}\".format(score, 1/opt.T))\n",
    "    \n",
    "    # Lower the temperature every 5 iterations\n",
    "    if i%5==4:\n",
    "        opt.T *= 1.1\n",
    "        \n",
    "# Optimized segment libraries\n",
    "prefixLibrary, suffixLibrary = bestLib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
