{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    with open(\"./encoding.pkl\", 'rb') as fin:\n",
    "        onehot = pickle.load(fin)\n",
    "        \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    ranges = []\n",
    "    for aa, v in data:\n",
    "        extra = 40-len(aa)\n",
    "        r = math.floor(extra/2)\n",
    "        l = math.ceil(extra/2)\n",
    "        aa = np.array([onehot[c] for c in ('J'*r) + aa + ('J'*l)]).T\n",
    "        rng = (40-l-20, r)\n",
    "        \n",
    "        inputs.append(aa)\n",
    "        labels.append(float(v))\n",
    "        ranges.append(rng)\n",
    "    return inputs, labels, ranges\n",
    "    \n",
    "def toTensorDataset(datalist):\n",
    "    torchds = torch.utils.data.TensorDataset(torch.tensor(np.concatenate([data[0] for data in datalist]), dtype = torch.float32),\n",
    "                                             torch.tensor(np.concatenate([data[1] for data in datalist]), dtype = torch.float32),\n",
    "                                             torch.tensor(np.concatenate([data[2] for data in datalist]), dtype = torch.int64))\n",
    "    return torchds\n",
    "\n",
    "def getData(fname):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        train, test = pickle.load(fin)\n",
    "        \n",
    "    train = [process(z) for z in train]\n",
    "    test = process(test)\n",
    "    test = torch.utils.data.DataLoader(toTensorDataset([test]), batch_size=1000, shuffle=False, num_workers=2, drop_last=False)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, dilation):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
    "        self.conv1_bn = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
    "        self.conv2_bn = nn.BatchNorm1d(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu( self.conv1_bn( self.conv1(x) ) )\n",
    "        return self.conv2_bn( self.conv2(x1) ) + x\n",
    "\n",
    "class EmbedderNet(nn.Module):\n",
    "    def __init__(self, length, channels, outchannels):\n",
    "        super(EmbedderNet, self).__init__()\n",
    "        self.length = length\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(2, 2, ceil_mode = True)\n",
    "        self.conv = nn.Conv1d(40, channels, 1, 1, 0)\n",
    "        self.conv_bn = nn.BatchNorm1d(channels)\n",
    "        \n",
    "        self.block1 = ResBlock(channels, 1)\n",
    "        self.block2 = ResBlock(channels, 1)\n",
    "        self.block3 = ResBlock(channels, 1)\n",
    "        self.block4 = ResBlock(channels, 1)\n",
    "        self.block5 = ResBlock(channels, 1)\n",
    "        \n",
    "        self.embed_1 = nn.Linear( int( channels*math.ceil(math.ceil(self.length/2)/2) ) , 128)\n",
    "        self.embed_2 = nn.Linear(128, outchannels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        x = F.relu( self.conv_bn( self.conv(x) ) )\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool( self.block4(x) )\n",
    "        x = self.pool( self.block5(x) ).view(batchsize,-1)\n",
    "        return self.embed_2( F.relu( self.embed_1(x) ) )\n",
    "    \n",
    "class Predictor_Dot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor_Dot, self).__init__()\n",
    "        self.embedPrefix = EmbedderNet(10, 64, 16)\n",
    "        self.embedSuffix = EmbedderNet(10, 64, 16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        prefix = x[:,:,:10]\n",
    "        suffix = x[:,:,10:]\n",
    "        return torch.sum(self.embedPrefix(prefix) * self.embedSuffix(suffix), dim = 1)\n",
    "    \n",
    "class Predictor_Plus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor_Plus, self).__init__()\n",
    "        self.embedPrefix = EmbedderNet(10, 64, 1)\n",
    "        self.embedSuffix = EmbedderNet(10, 64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        prefix = x[:,:,:10]\n",
    "        suffix = x[:,:,10:]\n",
    "        return self.embedPrefix(prefix) + self.embedSuffix(suffix)\n",
    "    \n",
    "class Predictor_Joint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor_Joint, self).__init__()\n",
    "        self.predictor = EmbedderNet(20, 64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomPadding(inputs, bounds):\n",
    "    bounds = bounds.permute((1,0))\n",
    "    indexes = np.random.randint(bounds[0], bounds[1]+1).reshape(-1,1)\n",
    "    gatherindex = torch.tensor(\n",
    "        np.array([i for i in range(20)]).reshape(1,-1) + indexes,\n",
    "        dtype = torch.int64 )\n",
    "    gatherindex = gatherindex.view(-1,1,20).repeat(1,40,1)\n",
    "    \n",
    "    result = torch.gather(inputs, 2, gatherindex)\n",
    "    return result\n",
    "\n",
    "def FixedPadding(inputs, bounds):\n",
    "    bounds = bounds.permute((1,0))\n",
    "    indexes = np.ones(bounds[0].shape[0]).reshape(-1,1)*10\n",
    "    gatherindex = torch.tensor(\n",
    "        np.array([i for i in range(20)]).reshape(1,-1) + indexes,\n",
    "        dtype = torch.int64 )\n",
    "    gatherindex = gatherindex.view(-1,1,20).repeat(1,40,1)\n",
    "    \n",
    "    result = torch.gather(inputs, 2, gatherindex)\n",
    "    return result\n",
    "\n",
    "def validate(net, loader, paddingFunction):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    if len(loader) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, position=0, leave=True)\n",
    "        for (data, target, bounds) in pbar:\n",
    "            data = paddingFunction(data, bounds).cuda()\n",
    "            predictions.append( net(data).cpu().numpy().reshape(-1) )\n",
    "            targets.append( target.numpy().reshape(-1) )\n",
    "        predictions = np.concatenate(predictions)\n",
    "        targets = np.concatenate(targets)\n",
    "        return predictions, targets\n",
    "\n",
    "def getTrainingValidationSplit(data, i):\n",
    "    t = toTensorDataset( data[:i] + data[i+1:] )\n",
    "    v = toTensorDataset([data[i]])\n",
    "    trainloader = torch.utils.data.DataLoader(t, batch_size=100, shuffle=True, num_workers=2, drop_last=True)\n",
    "    valloader = torch.utils.data.DataLoader(v, batch_size=1000, shuffle=False, num_workers=2, drop_last=False)\n",
    "    return trainloader, valloader\n",
    "\n",
    "# epochs: Number of training epochs\n",
    "# train_loader: Dataloader for training data\n",
    "# validation_loader: Dataloader for validation data\n",
    "# modelname: Descriptive name for model. Can be anything\n",
    "# architecture: Predictor_Dot, Predictor_Joint, or Predictor_Plus\n",
    "# paddingFunction: RandomPadding or FixedPadding\n",
    "def trainNet(epochs, train_loader, validation_loader, modelname, architecture, paddingFunction):\n",
    "    net = architecture().cuda()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    lossfunc = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    loss_trace = []\n",
    "    validation_trace = []\n",
    "    bestr = float('-inf')\n",
    "    for i in range(epochs):\n",
    "        losses = []\n",
    "        net = net.train()\n",
    "        pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        for (data, target, bounds) in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            data = paddingFunction(data, bounds).cuda()\n",
    "            output = net(data)\n",
    "            \n",
    "            loss = lossfunc(output.reshape(-1), target.cuda() )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss = loss.item()\n",
    "            losses.append(current_loss)\n",
    "            \n",
    "            pbar.set_description(\"Batch {}: loss_avg = {:.5f}, loss_now = {:.5f}\".format(i, np.mean(losses), current_loss ))\n",
    "        net = net.eval()\n",
    "        valx, valy = validate(net, validation_loader, paddingFunction)\n",
    "        r = stats.pearsonr(valx, valy)[0]\n",
    "        validation_trace.append(r)\n",
    "        print (\"Epoch {}: r={}\".format(i,r))\n",
    "        \n",
    "        if r > bestr:\n",
    "            torch.save(net.state_dict(), \"./weights/{}_best.pt\".format(modelname))\n",
    "            bestr = r\n",
    "        loss_trace.append(losses)\n",
    "        \n",
    "    net = architecture()\n",
    "    net.load_state_dict(torch.load( \"./weights/{}_best.pt\".format(modelname), map_location='cpu' ))\n",
    "    net.eval()\n",
    "    return net, loss_trace, validation_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "training_splits, test_loader = getData(\"./data/ranibizumab_log_enrichment.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, losses, performance_trace = trainNet(\n",
    "    2, *getTrainingValidationSplit(training_splits, 0), \"testrun\", Predictor_Dot, RandomPadding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "predictions, values = validate(model[0].cuda(), test_loader, FixedPadding)\n",
    "stats.pearsonr(predictions, values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
